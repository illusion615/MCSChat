# MCSChat - Advanced Chatbot Interface

A sophisticated chatbot user interface that demonstrates how to create a customized chat experience with multiple AI backends, featuring Microsoft Copilot Studio integration via DirectLine API, AI companion analysis, and local Ollama model support with real-time streaming capabilities.

## üöÄ Features

### Core Functionality
- **Multi-Agent Management**: Support for multiple chatbot agents with## üö¶ Troubleshooting

### Common Issues
- **CORS Errors**: Use the included `ollama-proxy.js` for local models
- **Connection Failures**: Verify DirectLine secrets and network connectivity
- **File Upload Issues**: Check file size limits and format restrictions
- **Streaming Problems**: Ensure proper API provider configuration
- **Message Ordering Issues**: Check browser console for timestamp comparison logs
- **Citation Display Problems**: Verify message metadata and entity structure

### Debug Mode
Enable detailed logging by opening browser developer tools. All operations are logged with timestamps and context.

### Message Ordering Debugging
If messages appear out of chronological order:
1. Open browser developer console (F12)
2. Look for "Inserting message with timestamp" logs
3. Check timestamp comparison details with human-readable times
4. Verify timestamp parsing and validation messages
5. Review final insertion position decisions

### Citation and Reference Issues
If citations don't display properly:
1. Check console for "hasCitations" and "addCitationsSection" logs
2. Verify activity object contains proper entities/metadata
3. Ensure inline references [1], [2] are properly styled
4. Check for citation positioning outside message bubblesl configurations
- **AI Companion Analysis**: Real-time conversation analysis with performance metrics and insights
- **Secure Credential Storage**: AES-256 encrypted storage for API keys and secrets
- **Real-time Streaming**: Support for both simulated and genuine streaming responses
- **File Upload Support**: Drag-and-drop file uploads with preview and attachment handling
- **Chat History Management**: Persistent conversation history with session management
- **Responsive Design**: Modern, mobile-friendly interface with customizable font sizes
- **Enhanced Settings Panel**: Organized configuration interface with navigation and sections
- **Chronological Message Ordering**: Robust timestamp-based message ordering with error handling
- **Citation System**: Enhanced reference handling with clickable inline citations and proper positioning

### AI Backend Integrations
- **Microsoft Copilot Studio**: Full DirectLine API integration with adaptive cards and suggested actions
- **Local Ollama Models**: Native support for local LLM models with automatic model discovery
- **Multi-Provider API Support**: OpenAI GPT, Anthropic Claude, Azure OpenAI support
- **CORS-Free Local Access**: Built-in proxy server for seamless local model connectivity

### AI Companion Features
- **Performance Analytics**: Real-time KPI tracking (Accuracy, Helpfulness, Completeness, etc.)
- **Conversation Analysis**: AI-powered insights and conversation summarization
- **Agent Performance Monitoring**: Visual metrics with trend analysis and scoring
- **Interactive Analysis**: Quick actions for analyzing responses and suggesting improvements
- **Contextual Insights**: Conversation-aware analysis with detailed explanations

### Advanced Features
- **Streaming Response Display**: Real-time message rendering with typing indicators
- **Adaptive Card Rendering**: Rich card display for complex bot responses
- **Suggested Actions**: Interactive quick-reply buttons for enhanced UX
- **Connection Testing**: Built-in connectivity testing for all configured backends
- **Session Management**: Automatic session handling with conversation continuity
- **Error Handling**: Comprehensive error reporting and recovery mechanisms
- **Font Customization**: Adjustable font sizes for agent and AI companion chats
- **Consolidated Initialization**: Unified startup process for consistent user experience
- **Dual Chat Windows**: Support for both Agent Chat (middle panel) and AI Companion Chat (right panel)
- **Message Display Options**: User-configurable bubble vs full-width message display modes
- **Enhanced Citation Handling**: Proper positioning of references with inline styling and click navigation
- **Robust Timestamp Management**: Advanced message ordering with validation and error recovery

### Developer Features
- **Modular Architecture**: Clean separation of concerns with reusable components
- **Event-Driven Design**: Comprehensive event handling for all user interactions
- **Debug Console Integration**: Detailed logging for troubleshooting and development
- **Configuration Management**: Centralized settings with organized navigation
- **Extension Ready**: Pluggable architecture for adding new AI providers

## üìã Quick Start

### Basic Setup
1. **Get DirectLine Secret**: Find your secret in Microsoft Copilot Studio web channel security
   ![DirectLine Secret](image-2.png)

2. **Open the Application**: Launch `index.html` in your browser
   ![Application Interface](image-5.png)

3. **Configure Agent**: Click setup button, navigate to Agent Management section, input your secret, test and save configuration
   ![Setting Panel](image-6.png)

4. **Enable AI Companion** (Optional): In settings, go to AI Companion section to enable analysis features
   ![alt text](image-7.png)

5. **Customize Appearance** (Optional): Adjust font sizes in the Appearance section for optimal readability
   ![alt text](image-8.png)

6. **Start Chatting**: Begin interacting with your configured agent
   ![Chat Interface](image-3.png)

### Legacy Version
- For the original monolithic version, use `index-legacy.html` with `chat-legacy.js`
- The current version (`index.html`) is the recommended approach for all deployments

### Local Ollama Setup
1. **Start CORS Proxy**: `node ollama-proxy.js` (handles browser CORS restrictions)
2. **Configure Ollama**: In settings AI Companion section, select "Local Ollama" as API provider
3. **Test Connection**: Use built-in connection test to verify setup
4. **Select Model**: Choose from automatically discovered local models

### AI Companion Configuration
1. **Enable AI Companion**: In settings, check "Enable AI Companion" in the AI Companion section
2. **Select Provider**: Choose from OpenAI GPT, Anthropic Claude, Azure OpenAI, or Local Ollama
3. **Enter Credentials**: Add your API key for cloud providers or configure Ollama URL
4. **Test Connection**: Verify the AI companion connection before use
5. **Start Analysis**: Use quick actions or custom prompts for conversation analysis

### Development Server Setup
```bash
# Start the chat application server
node chat-server.js
# Access at http://localhost:8080

# Start the Ollama CORS proxy (if using local models)
node ollama-proxy.js
# Proxy runs at http://localhost:3001
```

## üõ†Ô∏è Technical Architecture

### Frontend Components
- **Chat Interface**: Real-time messaging with streaming support and dual-panel layout
- **Agent Manager**: Multi-agent configuration and switching with status monitoring
- **AI Companion Panel**: Performance analytics, conversation analysis, and KPI tracking
- **Enhanced Settings Panel**: Organized configuration with navigation (Agent Management, AI Companion, Appearance)
- **File Handler**: Drag-and-drop uploads with preview
- **Stream Manager**: Progressive response rendering system
- **Security Layer**: Client-side encryption for sensitive data
- **Font Customization**: User-configurable font sizes for optimal readability
- **Message Renderer**: Advanced chronological ordering with timestamp validation and citation handling
- **Window Context Manager**: Dynamic targeting for Agent Chat vs AI Companion Chat windows
- **Citation System**: Enhanced reference display with inline styling and proper positioning
- **Debug Console**: Comprehensive logging system for troubleshooting message ordering and rendering

### Backend Integrations
- **DirectLine Client**: Microsoft Bot Framework connectivity
- **Ollama Interface**: Local model API integration  
- **Proxy Server**: CORS-compliant local model access
- **Storage Engine**: Encrypted localStorage with key management

### Security Features
- **AES-256 Encryption**: All credentials encrypted at rest
- **Key Derivation**: Secure key generation and management
- **Session Security**: Temporary credential handling
- **CORS Protection**: Secure cross-origin request handling

## üìÅ Project Structure

```
MCSChat/
‚îú‚îÄ‚îÄ index.html              # Main application interface (modular architecture)
‚îú‚îÄ‚îÄ index-legacy.html       # Legacy application interface (monolithic)
‚îú‚îÄ‚îÄ chat-legacy.js          # Original monolithic application logic (backup)
‚îú‚îÄ‚îÄ styles.css              # Application styling and responsive design
‚îú‚îÄ‚îÄ ollama-proxy.js         # CORS proxy server for local Ollama access
‚îú‚îÄ‚îÄ chat-server.js          # Development HTTP server
‚îú‚îÄ‚îÄ streaming-example.js    # Reference implementation for streaming APIs
‚îú‚îÄ‚îÄ images/                 # UI assets and screenshots
‚îú‚îÄ‚îÄ src/                    # Modular source code architecture
‚îÇ   ‚îú‚îÄ‚îÄ main.js             # Application entry point and initialization
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ application.js  # Main application controller and orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ managers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agentManager.js # Multi-agent configuration and management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sessionManager.js # Chat session and history management
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ directLineManager.js # DirectLine API integration and connection
‚îÇ   ‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ messageRenderer.js # Message display, adaptive cards, streaming
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aiCompanion.js  # Ollama integration and AI companion features
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ encryption.js   # AES-256-GCM encryption utilities
‚îÇ       ‚îú‚îÄ‚îÄ secureStorage.js # Encrypted localStorage wrapper
‚îÇ       ‚îú‚îÄ‚îÄ domUtils.js     # DOM manipulation helpers with error handling
‚îÇ       ‚îî‚îÄ‚îÄ helpers.js      # General utility functions and ID generation
‚îî‚îÄ‚îÄ README.md              # This documentation
```

## üèóÔ∏è Module Architecture

### Core Components
- **Application Controller** (`src/core/application.js`): Central orchestrator managing all modules and coordinating application lifecycle
- **Main Entry Point** (`src/main.js`): Application initialization, error handling, and DOM ready management

### Manager Layer
- **AgentManager**: Handles multiple bot configurations, agent switching, and credential management
- **SessionManager**: Manages chat sessions, message history, conversation state, and storage

### Service Layer  
- **DirectLineManager**: Microsoft Bot Framework DirectLine API integration with connection handling

### UI Layer
- **MessageRenderer**: Message display, adaptive card rendering, streaming text, chronological ordering, citation handling, and dual-window support

### AI Integration
- **AICompanion**: Ollama model integration, conversation analysis, KPI tracking, and AI-powered insights

### Utility Layer
- **Encryption**: AES-256-GCM encryption for secure credential and conversation storage
- **SecureStorage**: Encrypted localStorage wrapper with key management
- **DOMUtils**: Safe DOM manipulation with comprehensive error handling
- **Helpers**: Common utilities (debounce, throttle, ID generation, formatting)

### Design Patterns
- **ES6 Modules**: Clean import/export system with explicit dependencies
- **Singleton Pattern**: Shared instances for managers and services
- **Event-Driven Architecture**: Modular communication through custom events
- **Separation of Concerns**: Each module has a single, well-defined responsibility

## üîß Configuration Options

### Agent Settings (Agent Management Section)
- **Agent Name**: Friendly identifier for the bot
- **DirectLine Secret**: Azure Bot Service authentication key
- **Connection Status**: Real-time connectivity monitoring
- **Streaming Options**: Enable/disable streaming response simulation

### AI Companion Settings (AI Companion Section)
- **Provider Selection**: Choose between OpenAI GPT, Anthropic Claude, Azure OpenAI, or Local Ollama
- **API Configuration**: Secure storage of API keys and credentials
- **Model Selection**: Automatic discovery and selection of available models
- **Connection Testing**: Built-in connectivity verification

### Appearance Settings (Appearance Section)
- **Agent Chat Font Size**: Customizable font size (10-20px) for agent conversation messages
- **AI Companion Font Size**: Separate font size control (8-16px) for companion analysis
- **Real-time Preview**: Instant font size changes with live preview

### Streaming Options
- **Simulation Mode**: Progressive display of complete responses
- **Real Streaming**: Genuine streaming from compatible APIs
- **Provider Selection**: Choose between DirectLine, Ollama, OpenAI, etc.

### File Upload Settings
- **Supported Formats**: Images, documents, and text files
- **Size Limits**: Configurable upload restrictions
- **Preview Mode**: Automatic file preview before sending

## üîå API Integration Guide

### Adding New Providers
1. **Implement Provider Interface**: Follow the `sendToOllama` pattern
2. **Add UI Configuration**: Extend the provider selection dropdown
3. **Handle Streaming**: Implement progressive response display
4. **Add Connection Testing**: Create provider-specific health checks

### Custom Streaming Implementation
```javascript
async function handleCustomStreaming(message) {
    // Your streaming implementation here
    // Follow the pattern in handleOllamaStreaming()
}
```

## ÔøΩ Deployment Guide

### Development Environment
1. **Prerequisites**
   - Python 3.7+ for local HTTP server
   - Modern web browser with ES6 support
   - Text editor or IDE

2. **Quick Start**
   ```bash
   # Clone the repository
   git clone https://github.com/illusion615/MCSChat.git
   cd MCSChat
   
   # Start development server
   python -m http.server 8000
   
   # Access application
   # Navigate to http://localhost:8000
   ```

### Production Deployment Options

#### Option 1: Static Web Hosting (Recommended for most users)

**Netlify Deployment**
1. Fork or download this repository
2. Connect your GitHub account to Netlify
3. Deploy directly from GitHub
4. Configure custom domain (optional)

**Vercel Deployment**
1. Install Vercel CLI: `npm i -g vercel`
2. Run `vercel` in the project directory
3. Follow the prompts to deploy

**GitHub Pages**
1. Enable GitHub Pages in repository settings
2. Select source branch (main/master)
3. Access via `https://yourusername.github.io/MCSChat`

#### Option 2: Traditional Web Servers

**Apache Configuration**
```apache
<VirtualHost *:80>
    ServerName your-domain.com
    DocumentRoot /path/to/MCSChat
    
    # Enable CORS for API calls
    Header always set Access-Control-Allow-Origin "*"
    Header always set Access-Control-Allow-Methods "GET, POST, OPTIONS"
    Header always set Access-Control-Allow-Headers "Content-Type, Authorization"
    
    # Security headers
    Header always set X-Frame-Options DENY
    Header always set X-Content-Type-Options nosniff
    Header always set X-XSS-Protection "1; mode=block"
</VirtualHost>
```

**Nginx Configuration**
```nginx
server {
    listen 80;
    server_name your-domain.com;
    root /path/to/MCSChat;
    index index-modular.html;
    
    # CORS configuration
    add_header Access-Control-Allow-Origin *;
    add_header Access-Control-Allow-Methods "GET, POST, OPTIONS";
    add_header Access-Control-Allow-Headers "Content-Type, Authorization";
    
    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    
    location / {
        try_files $uri $uri/ /index.html;
    }
}
```

**IIS Configuration (Windows)**
1. Copy files to `C:\inetpub\wwwroot\MCSChat`
2. Create web.config:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <system.webServer>
        <httpProtocol>
            <customHeaders>
                <add name="Access-Control-Allow-Origin" value="*" />
                <add name="X-Frame-Options" value="DENY" />
            </customHeaders>
        </httpProtocol>
        <defaultDocument>
            <files>
                <add value="index.html" />
            </files>
        </defaultDocument>
    </system.webServer>
</configuration>
```

#### Option 3: Docker Deployment

**Dockerfile**
```dockerfile
FROM nginx:alpine
COPY . /usr/share/nginx/html/
COPY nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

**Docker Commands**
```bash
# Build image
docker build -t mcschat .

# Run container
docker run -p 8080:80 mcschat

# Access application at http://localhost:8080
```

**Docker Compose**
```yaml
version: '3.8'
services:
  mcschat:
    build: .
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
```

### Environment Configuration

#### Security Considerations
1. **API Key Management**
   - Never commit API keys to version control
   - Use environment-specific configuration
   - Consider using Azure Key Vault or AWS Secrets Manager for production

2. **HTTPS Configuration**
   - Always use HTTPS in production
   - Configure SSL certificates (Let's Encrypt recommended)
   - Update CORS policies for production domains

3. **Content Security Policy**
   ```html
   <meta http-equiv="Content-Security-Policy" 
         content="default-src 'self'; 
                  script-src 'self' 'unsafe-inline'; 
                  style-src 'self' 'unsafe-inline';
                  connect-src 'self' https://api.openai.com https://api.anthropic.com https://*.openai.azure.com http://localhost:11434">
   ```

#### Performance Optimization
1. **Enable Compression**
   - Gzip/Brotli for static assets
   - Minify CSS and JavaScript files

2. **Caching Strategy**
   ```nginx
   location ~* \.(css|js|png|jpg|jpeg|gif|ico|svg)$ {
       expires 1y;
       add_header Cache-Control "public, immutable";
   }
   ```

3. **CDN Integration**
   - Use CDN for static assets
   - Consider edge locations for global users

### Production Checklist
- [ ] API keys configured and secured
- [ ] HTTPS enabled with valid SSL certificate
- [ ] Security headers configured
- [ ] CORS policies set for production domains
- [ ] Error monitoring implemented
- [ ] Backup strategy in place
- [ ] Performance monitoring configured
- [ ] Content Security Policy applied
- [ ] File upload limits configured
- [ ] Rate limiting implemented (if applicable)

### Monitoring and Maintenance
1. **Health Checks**
   - Monitor API endpoint availability
   - Set up uptime monitoring
   - Configure alerting for failures

2. **Log Management**
   - Enable browser console logging in development
   - Implement server-side logging for production
   - Monitor for JavaScript errors

3. **Updates and Maintenance**
   - Regularly update dependencies
   - Monitor for security vulnerabilities
   - Test new API provider integrations

## ÔøΩüö¶ Troubleshooting

### Common Issues
- **CORS Errors**: Use the included `ollama-proxy.js` for local models
- **Connection Failures**: Verify DirectLine secrets and network connectivity
- **File Upload Issues**: Check file size limits and format restrictions
- **Streaming Problems**: Ensure proper API provider configuration

### Debug Mode
Enable detailed logging by opening browser developer tools. All operations are logged with timestamps and context.

---

## üìÖ Changelog

### Version 3.3.0 (2025-07-30 - Enhanced Citation System)
**Major Citation System Overhaul:**
- üÜï **Inline Citation Integration**: Citations now appear directly within message content instead of separate containers
- üÜï **Streamlined Citation Display**: Clean "Sources: [1] document.pdf (page 5), [2] another.pdf" format
- üÜï **Improved Message Layout**: Fixed horizontal citation layout issues for proper vertical integration
- üÜï **Enhanced Citation Rendering**: Simplified DOM structure with direct content appending for better performance
- üÜï **Side Browser Integration**: Enhanced citation links with CSP-aware external browser fallbacks

**Technical Improvements:**
- üîß **Fixed Citation Positioning**: Resolved messageContainer vs messageDiv targeting for proper inline placement
- üîß **Optimized Citation Rendering**: Removed complex column layouts in favor of simple content integration
- üîß **Improved CSS Management**: Cleaned up unused simplified citation styles for better maintainability
- üîß **Enhanced Debug Support**: Better error handling for citation rendering with detailed console logging
- üîß **DOM Performance**: Reduced citation-related DOM complexity by ~60% for faster rendering

**User Experience:**
- ‚ö° **Natural Citation Flow**: Citations now feel like an integral part of the message content
- ‚ö° **Consistent Message Styling**: Uniform appearance between messages with and without citations
- ‚ö° **Improved Readability**: Better visual hierarchy with citations at the end of message content
- ‚ö° **Responsive Design**: Citations adapt naturally to different screen sizes and layouts

### Version 3.2.2 (Current - Message Ordering & Citation Enhancements)
**Critical Bug Fixes:**
- üêõ **Message Chronological Ordering**: Fixed timestamp comparison algorithm to ensure messages display in correct chronological order (oldest to newest)
- üêõ **Timestamp Parsing**: Enhanced timestamp validation with better error handling for malformed or missing timestamps
- üêõ **Citation Positioning**: Fixed reference sections to display at the bottom of messages instead of inline
- üêõ **Agent Metadata Alignment**: Corrected agent meta information alignment with message bubbles

**UI/UX Improvements:**
- ‚ö° **Enhanced Reference Handling**: Improved inline reference styling with clickable [1], [2] style citations
- ‚ö° **Side Browser Restoration**: Restored missing side browser option in settings panel
- ‚ö° **Message Display Options**: Added setting to allow users to choose between bubble and full-width display for agent messages
- ‚ö° **Dual Chat Window Support**: Enhanced message renderer to properly handle both Agent Chat (middle panel) and AI Companion Chat (right panel)

**Technical Enhancements:**
- üîß **Robust Timestamp Handling**: Added comprehensive error handling for timestamp parsing with fallback to current time
- üîß **Message Insertion Algorithm**: Improved `insertMessageInOrder()` method with detailed debug logging and tie-breaking logic
- üîß **Citation System**: Enhanced citation detection and rendering with proper positioning outside message bubbles
- üîß **Window Context Management**: Added `setTargetWindow()` and `getCurrentWindowId()` methods for better chat window targeting
- üîß **Debug Capabilities**: Extensive logging for message ordering troubleshooting with human-readable timestamps

**Debugging Features:**
- üìä **Enhanced Debug Logging**: Detailed timestamp comparison logs with human-readable time formats
- üìä **Message Insertion Tracking**: Step-by-step logging of message positioning decisions
- üìä **Error Recovery**: Graceful handling of invalid timestamps with appropriate fallback mechanisms
- üìä **Tie-Breaking Logic**: Smart handling of messages with identical or very similar timestamps

### Version 3.2.1 (Unified AI Companion Architecture)
**Code Consolidation & Bug Fixes:**
- üîß **Unified Streaming Architecture**: Consolidated all AI provider streaming implementations to use shared markdown processing
- üîß **Markdown Rendering Fix**: Fixed Azure OpenAI and Anthropic Claude responses to properly render markdown like Ollama
- üîß **Code Deduplication**: Eliminated duplicate message creation and content processing logic across all providers
- üîß **Improved Maintainability**: Created reusable helper methods for message handling and content processing
- üêõ **Consistent User Experience**: All AI providers now have identical formatting and rendering capabilities

**Technical Improvements:**
- ‚ö° **Unified Message Processing**: Single `processMarkdownContent()` method for all providers
- ‚ö° **Consolidated Streaming**: Shared `initializeStreamingMessage()`, `updateStreamingContent()`, and `finalizeMessage()` methods
- ‚ö° **Cleaner Architecture**: Reduced code duplication by ~40% in AI companion module
- ‚ö° **Better Error Handling**: Consistent error handling across all streaming implementations

### Version 3.2.0 (AI Companion & Enhanced UX)
**Major Features Added:**
- üÜï **AI Companion Panel**: Complete conversation analysis system with real-time KPI tracking
- üÜï **Performance Analytics**: Accuracy, Helpfulness, Completeness, Human-likeness, and Efficiency metrics
- üÜï **Enhanced Settings Panel**: Organized navigation with Agent Management, AI Companion, and Appearance sections
- üÜï **Font Customization**: User-configurable font sizes for agent chat and AI companion messages
- üÜï **Consolidated Initialization**: Unified startup process for "New Chat" and page refresh
- üÜï **Multi-Provider AI Support**: OpenAI GPT, Anthropic Claude, Azure OpenAI, and Local Ollama integration

**UI/UX Improvements:**
- ‚ö° **Wider Settings Modal**: Extended to 800px width with responsive navigation
- ‚ö° **Section-Based Configuration**: Logical grouping of related settings with quick navigation
- ‚ö° **Real-time Font Preview**: Instant font size changes with live preview
- ‚ö° **Improved AI Companion Styling**: Consistent message styling with proper user/bot alignment
- ‚ö° **Enhanced Mobile Support**: Responsive navigation that adapts to smaller screens
- ‚ö° **Better Visual Hierarchy**: Section titles, organized layouts, and improved spacing

**Technical Enhancements:**
- üîß **Event-Driven Navigation**: Smooth section switching with active state management
- üîß **Persistent Settings**: All customizations saved to localStorage with encryption
- üîß **Modular AI Integration**: Clean separation of AI providers with extensible architecture
- üîß **Improved Error Handling**: Better user feedback and recovery mechanisms
- üîß **Performance Optimization**: Efficient DOM updates and reduced re-renders

### Version 3.1.0 (2025-07-27)
**Added:**
- üÜï **AI Companion Integration**: Conversation analysis and insights powered by multiple AI providers
- üÜï **KPI Tracking System**: Real-time performance metrics with visual indicators
- üÜï **Quick Analysis Actions**: One-click conversation analysis, summarization, and title suggestions
- üÜï **Conversation Context**: Automatic context inclusion for AI companion analysis
- üÜï **Interactive Metrics**: Clickable KPI items with detailed explanations and calculations

**Improved:**
- ‚ö° **Dual-Panel Layout**: Side-by-side agent chat and AI companion analysis
- ‚ö° **Responsive Design**: Adaptive layout for different screen sizes and orientations
- ‚ö° **Message Styling**: Enhanced visual distinction between user and bot messages
- ‚ö° **Settings Organization**: Improved configuration flow with better user guidance

### Version 3.0.0 (2025-07-26 - Modular Architecture)
**Major Refactoring:**
- üîÑ Complete modular architecture implementation with ES6 modules
- üîÑ Separated monolithic `chat.js` (3799 lines) into focused, maintainable modules
- üîÑ Implemented clean separation of concerns with utils/, managers/, services/, ui/, ai/, and core/ layers
- üîÑ Added singleton pattern for shared instances and improved state management
- üîÑ Created new `index-modular.html` as the recommended entry point
- üîÑ Preserved legacy functionality in `chat-legacy.js` for backward compatibility

**Improved:**
- ‚ö° Enhanced maintainability with smaller, focused modules (50-400 lines each)
- ‚ö° Better error handling and debugging capabilities across all modules
- ‚ö° Improved code reusability and testability through modular design
- ‚ö° More intuitive project structure following modern JavaScript best practices

### Version 2.1.0 (2025-07-26)
**Added:**
- üÜï Native Ollama integration with automatic model discovery
- üÜï CORS proxy server for browser-based local model access
- üÜï Multi-provider API framework (OpenAI, Anthropic ready)
- üÜï Real-time streaming response display
- üÜï Enhanced error handling with specific troubleshooting guidance
- üÜï Automatic connection testing for all configured backends

**Improved:**
- ‚ö° Enhanced multi-agent management with visual status indicators
- ‚ö° Upgraded security with AES-256 encryption for all credentials
- ‚ö° Better responsive design for mobile and tablet devices
- ‚ö° Optimized DirectLine connection handling with retry logic

**Fixed:**
- üêõ Resolved CORS issues with local Ollama installations
- üêõ Fixed streaming message finalization edge cases
- üêõ Corrected file upload handling for large attachments
- üêõ Improved session management and conversation continuity

### Version 2.0.0 (2025-07-25)
**Added:**
- üÜï Multi-agent support with centralized management
- üÜï Encrypted credential storage with secure key management
- üÜï File upload with drag-and-drop support
- üÜï Streaming response simulation for enhanced UX
- üÜï Session-based chat history management
- üÜï Comprehensive error handling and user feedback

**Improved:**
- ‚ö° Modernized UI with improved accessibility
- ‚ö° Enhanced DirectLine integration with better connection stability
- ‚ö° Optimized chat history storage and retrieval
- ‚ö° Better responsive design for various screen sizes

### Version 1.0.0 (2025-07-24)
**Initial Release:**
- ‚ú® Basic DirectLine API integration
- ‚ú® Simple chat interface with message rendering
- ‚ú® Adaptive card support for rich bot responses
- ‚ú® Suggested actions for interactive conversations
- ‚ú® Basic configuration management

---

*Last Updated: July 30, 2025*
*Project maintained by: [MCSChat Contributors](https://github.com/illusion615/MCSChat)*